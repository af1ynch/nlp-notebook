统计语言模型
==========
    
    自然语言从它产生开始，逐渐演变成一种上下文相关的信息表达和传递的方式，因此让计算机处理自然语言，一个基本的问题就是
    为自然语言这种上下文相关的特性建立数学模型。这个数学模型就是在自然语言处理中常说的统计语言模型(Statistical Language Model)，
    它是今天所有自然语言处理的基础，并且广泛应用于各种自然语言处理应用当中。
    目前广泛使用的是n元语法(n-gram model)，这种模型构建简单、直接，但同时也因为数据缺乏而必须采取平滑(smoothing)算法。最近几年，
    随着深度学习技术的发展，深度神经网络语言模型更多的被研究人员采用。
    
## n元语法模型
    
    一个语言模型通常构建为字符串s的概率分布p(s)，这里p(s)试图反应的是字符串s作为一个句子出现的概率。例如，在一个刻画口语
    的语言模型中，如果一个人所说的话语每100个句子里大学有一句是Okay，则可认为p(Okay)=0.01。而对于句子“An apple ate the 
    chicken”我们可以认为其概率为0，因为几乎不会有人说这样的句子。需要注意的是，与语言学中不同，语言模型与句子是否合乎语法
    是没有关系的，即使一个句子完全合乎语法逻辑，我们仍然可以认为它出现的概率接近为零。
    对于一个由l个基元(“基元”可以为字、词或短语等)构成的句子s=w1w2···wl，其概率计算公式可表示为
    p(s)=p(w1)p(w2|w1)p(w3|w1w2)···p(wl|w1w2···wl-1)
    从计算上来看，第一个词的条件概率p(w1)很容易计算，第二个词的条件概率p(w2|w1)也还不太麻烦，第三个词的条件概率p(w3|w1,w2)
    已经非常难算了，因为它涉及到三个变量w1，w2，w3，每个变量的可能性都是一种语言字典的大小。到了最后一个词wl，条件概率的可能
    性太多了，无法估算。这可如何是好？
    俄国有个数学家马尔科夫(Andrew Markov)，他提出了一种偷懒但颇为有效的方法，就是每当遇到这种情况时，就假设任意一个词wi出现
    的概率只同它前面的词wi-1有关，于是问题就变得间大了。这种假设在数学上叫马尔科夫假设。现在求s出现的概率就变得简单了
    p(s)=p(w1)p(w2|w1)···p(wl|wl-1)
    公式对应的统计语言模型叫二元模型(Bi-gram model)。当然，也可假设一个词有前面的n-1个词决定，对应的模型稍微复杂些，被称为
    n元模型。
    
## 神经网络语言模型
    
    随着近几年深度学习的发展，神经网络相关研究越来越深入，神经网络语言模型(Neural Network Language Model, NNLM)越来越受到
    学术界和工业界的关注，NNLM最早由Bengio系统化提出并进行了深入研究[2003]，当然分开来看，distributed representation最早
    由Hinton提出[1985]，利用神经网络建模语言模型由Miikkulainen and Dyer，Xu and Rudnicky分别在1991和2000提出。NNLM依赖
    的一个核心概念就是Word Embedding。

### 深入阅读
----------------
[Yoshua Bengio. A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)   
[Statistical Language Models Based on Neural Networks](http://www.fit.vutbr.cz/~imikolov/rnnlm/google.pdf)   
[神经网络语言模型详解](https://mp.weixin.qq.com/s?__biz=MjM5ODIzNDQ3Mw==&mid=203735609&idx=1&sn=68f6e2ad560257869aeea09c59af1c1a&scene=2&from=timeline&isappinstalled=0&utm_source=open-open)   
[word2vec中的数学原理详解](http://blog.csdn.net/itplus/article/details/37969519)
